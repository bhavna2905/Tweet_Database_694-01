{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ee2e71",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d58ed9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "from datetime import datetime, timedelta\n",
    "from flask import Flask, render_template, request, url_for, redirect,flash\n",
    "import pymongo\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from markupsafe import escape\n",
    "from bson.objectid import ObjectId\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import mysql.connector\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from bson.json_util import dumps\n",
    "from bson import json_util\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341b7e3",
   "metadata": {},
   "source": [
    "## Connecting to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbd852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Table Users already exists dropping the table........\n",
      "\n",
      "The Table User has been created\n"
     ]
    }
   ],
   "source": [
    "#connecting to mysql\n",
    "# Establish a connection to MySQL\n",
    "cnx = mysql.connector.connect(user='root', password='password', host='localhost')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Create a new database\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS twitter_data_users_2\")\n",
    "\n",
    "# Use the new database\n",
    "cursor.execute(\"USE twitter_data_users_2\")\n",
    "cursor.execute(\"SHOW Tables Like 'users';\")\n",
    "result = cursor.fetchone()\n",
    "\n",
    "# Create a new table\n",
    "create_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    name VARCHAR(100),\n",
    "    screen_name VARCHAR(100),\n",
    "    location VARCHAR(1000),\n",
    "    followers_count INT,\n",
    "    friends_count INT,\n",
    "    statuses_count INT,\n",
    "    verified BOOLEAN,  \n",
    "    protected BOOLEAN,  \n",
    "    listed_count INT,  \n",
    "    created_at DATETIME\n",
    ")\n",
    "\"\"\"\n",
    "if result:\n",
    "    cursor.execute(\"DROP Table users;\")\n",
    "    print(\"The Table Users already exists dropping the table........\\n\")\n",
    "cursor.execute(create_table)\n",
    "print(\"The Table User has been created\")\n",
    "# Close the cursor and connection\n",
    "#cursor.close()\n",
    "#cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a03f4",
   "metadata": {},
   "source": [
    "## Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b9ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database exists. It will be deleted.\n",
      "Database connection established\n"
     ]
    }
   ],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "dbnames = client.list_database_names()\n",
    "if \"TweetsDB2\" in dbnames:\n",
    "    print(\"The database exists. It will be deleted.\")\n",
    "    client.drop_database(\"TweetsDB2\")\n",
    "db = client[\"TweetsDB2\"]\n",
    "col_names = db.list_collection_names()\n",
    "if \"Tweets_data_2\" in col_names:\n",
    "    print(\"Tweet Collection exists. It will be deleted.\")\n",
    "    db.Tweets_data.drop()\n",
    "tweets_coll = db[\"Tweets_data_2\"]\n",
    "print(\"Database connection established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71576fb5",
   "metadata": {},
   "source": [
    "## Initializing App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6984e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing app\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['SECRET_KEY'] = os.urandom(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e5fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, ObjectId):\n",
    "            return str(o)\n",
    "        if isinstance(o, datetime):\n",
    "            return o.isoformat()\n",
    "        return json.JSONEncoder.default(self, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d382",
   "metadata": {},
   "source": [
    "## Creating Cache and Creating Databases\n",
    "\n",
    "At the moment, this cache only includes storage for MongoDB -- the tweets database. All queries pertain to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe30b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache():\n",
    "    def __init__(self):\n",
    "        # Initialize the cache and the write counter\n",
    "        self.cache = {}\n",
    "        self.write_counter = 0\n",
    "\n",
    "        # Connect to the MongoDB server\n",
    "        self.client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "        self.db = self.client[\"TweetsDB2\"]\n",
    "        self.tweets_coll = self.db[\"tweets_coll\"]\n",
    "        \n",
    "        ## Connect to the MySQL server  (ADDED)\n",
    "        self.conn = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            user='root',\n",
    "            password='password',\n",
    "            database='twitter_data_users_2')\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def data_modifier(self, data):\n",
    "        # Convert the data to JSON format\n",
    "        res = {'res': data}\n",
    "        json_res = json.dumps(res, default=str, ensure_ascii=False).encode('utf8')\n",
    "        return json_res\n",
    "\n",
    "    def clear_20_percent(self):\n",
    "        # Clear the oldest 20% of keys in the cache\n",
    "        keys = sorted(self.cache.keys(), key=lambda x: self.cache[x][1])\n",
    "        target = math.ceil(len(keys)*0.2)\n",
    "\n",
    "        for i in range(target):\n",
    "            del self.cache[keys[i]]\n",
    "\n",
    "    def push_to_cache(self, key, data):\n",
    "        # Push data to the cache with a TTL of 3 days\n",
    "        data = self.data_modifier(data)\n",
    "        if sys.getsizeof(self.cache) < 10 * 1024 * 1024:  # 10 MB limit\n",
    "            self.cache[key] = (data, time.time())  # Set TTL to 3 days\n",
    "        else:\n",
    "            self.clear_20_percent()\n",
    "            self.cache[key] = (data, time.time())  # Set TTL to 3 days\n",
    "\n",
    "    def get_from_cache(self, key):\n",
    "        # Get data from the cache\n",
    "        try:\n",
    "            data, timestamp = self.cache[key]\n",
    "            ttl = 3 * 24 * 60 * 60\n",
    "            if time.time() - timestamp > ttl:\n",
    "                del self.cache[key]\n",
    "                return -1\n",
    "            return json.loads(data.decode())['res']\n",
    "        except KeyError:\n",
    "            return -1\n",
    "\n",
    "    def most_common_hashtags(self):\n",
    "        # Get the most recent date in the database\n",
    "        most_recent_doc = self.tweets_coll.find_one(sort=[(\"created_at\", -1)])\n",
    "        if most_recent_doc is None:\n",
    "            print(\"No documents found in the collection.\")\n",
    "            return []\n",
    "        if \"created_at\" not in most_recent_doc:\n",
    "            print(\"The 'created_at' field does not exist in the document.\")\n",
    "            return []\n",
    "        most_recent_date = most_recent_doc[\"created_at\"]\n",
    "        # Calculate the date 3 days before the most recent date\n",
    "        three_days_ago = most_recent_date - timedelta(days=3)\n",
    "        # Find the top 10 hashtags in the past 3 days\n",
    "        pipeline = [\n",
    "            {\"$match\": {\"created_at\": {\"$gte\": three_days_ago}}},\n",
    "            {\"$unwind\": \"$Hashtag\"},\n",
    "            {\"$group\": {\"_id\": \"$Hashtag\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            {\"$limit\": 10}\n",
    "        ]\n",
    "        result = self.tweets_coll.aggregate(pipeline)\n",
    "        most_common = list(result)\n",
    "        return most_common\n",
    "\n",
    "    def top_retweets(self):\n",
    "        # Get the most recent date in the database\n",
    "        most_recent_date = self.tweets_coll.find_one(sort=[(\"created_at\", -1)])[\"created_at\"]\n",
    "        # Calculate the date 3 days before the most recent date\n",
    "        three_days_ago = most_recent_date - timedelta(days=3)\n",
    "        # Find the top 10 tweets with the most retweets in the past 3 days\n",
    "        result = self.tweets_coll.find({\"created_at\": {\"$gte\": three_days_ago}}).sort(\"Retweet_Count\", -1).limit(10)\n",
    "        top_retweets = list(result)\n",
    "        return top_retweets\n",
    "    \n",
    "    def top10_tweets(self):\n",
    "        # Get the most recent date in the database\n",
    "        most_recent_date = self.tweets_coll.find_one(sort=[(\"created_at\", -1)])[\"created_at\"]\n",
    "        # Calculate the date 3 days before the most recent date\n",
    "        three_days_ago = most_recent_date - timedelta(days=3)\n",
    "        # Find the top 10 tweets with the most retweets in the past 3 days\n",
    "        # Find the top 10 original tweets (Retweet_ID = 0) by favorite count\n",
    "        top_original_tweets = self.tweets_coll.find({'Retweet_ID': 0}).sort('Favorite_Count', -1).limit(10)\n",
    "        return list(top_original_tweets)\n",
    "    \n",
    "\n",
    "    def most_active_users(self):\n",
    "        query = \"SELECT id, name, screen_name, statuses_count/(DATEDIFF(NOW(), created_at)+1) as tweet_frequency FROM users ORDER BY tweet_frequency DESC LIMIT 10\"\n",
    "        self.cursor.execute(query)\n",
    "        rows = self.cursor.fetchall()\n",
    "        result = []\n",
    "        for row in rows:\n",
    "            user_dict = {\n",
    "                \"id\": row[0],\n",
    "                \"name\": row[1],\n",
    "                \"screen_name\": row[2],\n",
    "                \"tweet_frequency\": row[3]\n",
    "            }\n",
    "            result.append(user_dict)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def update_cache_tweets(self):\n",
    "        # Update the cache with the top 10 hashtags and top 10 tweets with the most retweets\n",
    "        hashtags = self.most_common_hashtags()\n",
    "        self.push_to_cache(\"top_hashtags\", hashtags)\n",
    "        retweets = self.top_retweets()\n",
    "        self.push_to_cache(\"top_retweets\", retweets)\n",
    "        top10_tweets = self.top10_tweets()\n",
    "        self.push_to_cache(\"top10_tweets\", top10_tweets)\n",
    "        # Save all tweets within 3 days that used the most common hashtags\n",
    "        most_recent_date = self.tweets_coll.find_one(sort=[(\"created_at\", -1)])[\"created_at\"]\n",
    "        three_days_ago = most_recent_date - timedelta(days=3)\n",
    "        for hashtag in hashtags:\n",
    "            hashtag_tweets = self.tweets_coll.find({\"created_at\": {\"$gte\": three_days_ago}, \"Hashtag\": hashtag[\"_id\"]})\n",
    "            self.push_to_cache(f\"tweets_with_{hashtag['_id']}\", list(hashtag_tweets))\n",
    "        self.push_to_cache(\"most_recent_date\", most_recent_date)\n",
    "        self.push_to_cache(\"three_days_ago\", three_days_ago)\n",
    "        # Print the cache\n",
    "        print(\"Cache updated:\")\n",
    "        for key, value in self.cache.items():\n",
    "            print(f\"{key}\",f\"{value}\")\n",
    "\n",
    "        # Save the cache to a file using pickle\n",
    "        with open('cache.pkl', 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def update_cache_users(self):\n",
    "        # Update the cache with the results of the user queries\n",
    "        most_active_users = self.most_active_users()\n",
    "        self.push_to_cache(\"most_active_users\", most_active_users)\n",
    "        \n",
    "\n",
    "        # Print the cache\n",
    "        print(\"Cache updated:\")\n",
    "        for key, value in self.cache.items():\n",
    "            print(f\"{key}\",f\"{value}\")\n",
    "\n",
    "        # Save the cache to a file using pickle\n",
    "        with open('cache.pkl', 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def delete_tweets_cache(self):\n",
    "        keys_to_delete = [\"top_hashtags\", \"top_retweets\"]\n",
    "        keys_to_delete.extend(key for key in self.cache.keys() if key.startswith(\"tweets_with_\"))\n",
    "        for key in keys_to_delete:\n",
    "            if key in self.cache:\n",
    "                del self.cache[key]\n",
    "\n",
    "    def delete_users_cache(self):\n",
    "        keys_to_delete = [\"top_10_followers\", \"most_active_users\"]\n",
    "        for key in keys_to_delete:\n",
    "            if key in self.cache:\n",
    "                del self.cache[key]\n",
    "\n",
    "    def insert_into_mongo(self, data):\n",
    "        # Insert data into the MongoDB collection\n",
    "        self.tweets_coll.insert_one(data)\n",
    "        \n",
    "        # Update the write counter\n",
    "        self.write_counter += sys.getsizeof(data)\n",
    "        # If more than 100 MB of data has been written, update the cache and reset the counter\n",
    "        if self.write_counter >= 100 * 1024 * 1024:\n",
    "            self.delete_tweets_cache()\n",
    "            self.update_cache_tweets()\n",
    "            self.write_counter = 0\n",
    "            \n",
    "    def insert_into_mysql(self, query, data):\n",
    "        self.cursor.execute(query, data)\n",
    "        self.conn.commit()\n",
    "        self.write_counter += sys.getsizeof(data)\n",
    "        if self.write_counter >= 100 * 1024 * 1024:\n",
    "            self.delete_users_cache()\n",
    "            self.update_cache_users()\n",
    "            self.write_counter = 0\n",
    "\n",
    "# # Create a Cache object\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d6761f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def find_in_collection(tid):\n",
    "    record = None\n",
    "    record = cache.tweets_coll.find({'TweetID': tid})\n",
    "    if len(list(record)) > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def increment_retweet_count(tid):\n",
    "    cache.tweets_coll.update_one({'TweetID': tid}, {'$inc': {'Retweet_Count': 1}})\n",
    "\n",
    "def process_line(line):\n",
    "    line = line.strip()  # remove leading/trailing white spaces\n",
    "    if not line:  # checks if line is empty\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        json_object = json.loads(line)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if 'id' in json_object and find_in_collection(json_object['id']):\n",
    "            return\n",
    "\n",
    "        if json_object['text'].startswith('RT') and 'retweeted_status' in json_object:\n",
    "            RetweetID = json_object['retweeted_status']['id']\n",
    "            if find_in_collection(RetweetID):\n",
    "                increment_retweet_count(RetweetID)\n",
    "            else:\n",
    "                retweet_created_at = datetime.strptime(json_object['retweeted_status']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                retweeted_status = json_object['retweeted_status']\n",
    "                full_text = retweeted_status.get('extended_tweet', {}).get('full_text', retweeted_status['text'])\n",
    "                dict = {\n",
    "                    'created_at': retweet_created_at,\n",
    "                    'TweetID': retweeted_status['id'],\n",
    "                    'Id_str': retweeted_status['id_str'],\n",
    "                    'Text': retweeted_status['text'],\n",
    "                    'Full_Text': full_text,\n",
    "                    'Hashtag': list(map(lambda x: x[\"text\"], retweeted_status['entities']['hashtags'])),\n",
    "                    'UserID': retweeted_status['user']['id'],\n",
    "                    'Retweet_Count': 1,\n",
    "                    'Retweet_ID': 0,\n",
    "                    'Favorite_Count': retweeted_status['favorite_count']\n",
    "                }\n",
    "                cache.insert_into_mongo(dict)\n",
    "        else:\n",
    "            RetweetID = 0\n",
    "\n",
    "        tweet_created_at = datetime.strptime(json_object['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        dict = {\n",
    "            'created_at': tweet_created_at,\n",
    "            'TweetID': json_object['id'],\n",
    "            'Id_str': json_object['id_str'],\n",
    "            'Text': json_object['text'],\n",
    "            'Hashtag': list(map(lambda x: x[\"text\"], json_object['entities']['hashtags'])),\n",
    "            'UserID': json_object['user']['id'],\n",
    "            'Retweet_Count': 0,\n",
    "            'Retweet_ID': RetweetID,\n",
    "            'Favorite_Count': json_object['favorite_count']\n",
    "        }\n",
    "\n",
    "        cache.insert_into_mongo(dict)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "\n",
    "with open('corona-out-2', 'r') as f:\n",
    "    for line in f:\n",
    "        process_line(line)\n",
    "\n",
    "with open('corona-out-3', 'r') as f:\n",
    "    for line in f:\n",
    "        process_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b783fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corona-out-2\", 'r') as f:\n",
    "    # Process each line in the file\n",
    "    for line in f:\n",
    "        try:\n",
    "            # Parse the line as JSON\n",
    "            tweet = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # If a line cannot be parsed as JSON, skip it\n",
    "            continue\n",
    "\n",
    "        # Extract the user information from the tweet\n",
    "        user_info = tweet.get('user', {})\n",
    "\n",
    "        # Clean the name by removing all non-alphabet characters\n",
    "        cleaned_name = re.sub(r'[^a-zA-Z]', ' ', user_info.get('name'))\n",
    "\n",
    "        # Parse the Twitter datetime string to a datetime object\n",
    "        dt = datetime.strptime(user_info.get('created_at'), '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        \n",
    "        data =  (user_info.get('id'), cleaned_name, user_info.get('screen_name'), \n",
    "                user_info.get('location'), user_info.get('followers_count'), user_info.get('friends_count'), \n",
    "                user_info.get('statuses_count'), user_info.get('verified'), user_info.get('protected'), \n",
    "                user_info.get('listed_count'), dt)\n",
    "\n",
    "        # Prepare the SQL query\n",
    "        query = \"\"\"\n",
    "                INSERT IGNORE INTO users (id, name, screen_name, location, followers_count, friends_count, \n",
    "                statuses_count, verified, protected,listed_count, created_at)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "        cache.insert_into_mysql(query, data)\n",
    "        \n",
    "with open(\"corona-out-3\", 'r') as f:\n",
    "    \n",
    "    # Process each line in the file\n",
    "    for line in f:\n",
    "        try:\n",
    "            # Parse the line as JSON\n",
    "            tweet = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # If a line cannot be parsed as JSON, skip it\n",
    "            continue\n",
    "\n",
    "        # Extract the user information from the tweet\n",
    "        user_info = tweet.get('user', {})\n",
    "\n",
    "        # Clean the name by removing all non-alphabet characters\n",
    "        cleaned_name = re.sub(r'[^a-zA-Z]', ' ', user_info.get('name'))\n",
    "\n",
    "        # Parse the Twitter datetime string to a datetime object\n",
    "        dt = datetime.strptime(user_info.get('created_at'), '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        \n",
    "        data =  (user_info.get('id'), cleaned_name, user_info.get('screen_name'), \n",
    "                user_info.get('location'), user_info.get('followers_count'), user_info.get('friends_count'), \n",
    "                user_info.get('statuses_count'), user_info.get('verified'), user_info.get('protected'), \n",
    "                user_info.get('listed_count'), dt)\n",
    "\n",
    "        # Prepare the SQL query\n",
    "        query = \"\"\"\n",
    "                INSERT IGNORE INTO users (id, name, screen_name, location, followers_count, friends_count, \n",
    "                statuses_count, verified, protected,listed_count, created_at)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "        cache.insert_into_mysql(query, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036424e",
   "metadata": {},
   "source": [
    "## Tweet Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61584556",
   "metadata": {},
   "source": [
    "### Defining Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ab63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_hashtags_in_tweets_with_keyword(keyword, sorting_var=\"Retweet_Count\"):\n",
    "    regex = f\".*{keyword}.*\"  # Create a regex pattern to match the keyword in the hashtag\n",
    "    query = {\"Hashtag\": {\"$regex\": regex, \"$options\": \"i\"}}  # Case-insensitive regex match\n",
    "    tweets = db.tweets_coll.find(query, {\"Text\": 1, \"Retweet_Count\": 1, \"UserID\": 1, \"_id\": 1, \"created_at\": 1}).sort(sorting_var, -1)\n",
    "    result = []\n",
    "    for tweet in tweets:\n",
    "        tweet['_id'] = str(tweet['_id'])  # Convert ObjectId to string\n",
    "        result.append(tweet)\n",
    "    return result\n",
    "\n",
    "def search_tweets_by_keyword(keyword, sort_by='Retweet_Count'):\n",
    "    regex = f\".*{keyword}.*\"  # Create a regex pattern to match the keyword in the tweet text\n",
    "    query = {\"Text\": {\"$regex\": regex, \"$options\": \"i\"}}  # Case-insensitive regex match\n",
    "    tweets = db.tweets_coll.find(query)\n",
    "    sorted_tweets = sorted(tweets, key=lambda x: x.get(sort_by, 0), reverse=True)\n",
    "    results = []\n",
    "    for idx, tweet in enumerate(sorted_tweets, start=1):\n",
    "        result = {\n",
    "            'Text': tweet['Text'],\n",
    "            'Retweet_Count': tweet.get('Retweet_Count', 0),\n",
    "            'UserID': tweet['UserID'],\n",
    "            '_id': tweet['_id'],\n",
    "            'created_at': tweet['created_at']\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def tweets_10_fav():\n",
    "    agg_result = db.tweets_coll.aggregate([\n",
    "        {\"$sort\": {\"Favorite_Count\":-1}},\n",
    "        {\"$limit\": 10}\n",
    "    ])\n",
    "    output = list(agg_result)\n",
    "    return output\n",
    "\n",
    "def tweets_10_retweet():\n",
    "    agg_result = db.tweets_coll.aggregate([\n",
    "        {\"$sort\": {\"Retweet_Count\":-1}},\n",
    "        {\"$limit\": 10}\n",
    "    ])\n",
    "    output = list(agg_result)\n",
    "    return output\n",
    "\n",
    "def hashtag_10():\n",
    "    agg_result = db.tweets_coll.aggregate([\n",
    "        {\"$unwind\": \"$Hashtag\"},\n",
    "        {\"$group\": {\"_id\": \"$Hashtag\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": 10}\n",
    "    ])\n",
    "    output = list(agg_result)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01ef18",
   "metadata": {},
   "source": [
    "## User Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679bc58",
   "metadata": {},
   "source": [
    "### Defining Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67457d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_users_with_screen_name(word):\n",
    "    # This function finds users with screen names containing a specific word\n",
    "    connection = cnx\n",
    "    cursor = cnx.cursor()\n",
    "    query = \"SELECT * FROM users WHERE screen_name LIKE %s\"\n",
    "    cursor.execute(query, ('%' + word + '%',))\n",
    "    result = cursor.fetchall()\n",
    "    \n",
    "    # Create a dictionary to store the result\n",
    "    users_dict = {}\n",
    "    for row in result:\n",
    "        user_id = row[0]  # Assuming the first column is the user ID\n",
    "        user_details = {\n",
    "            'name': row[1],\n",
    "            'screen_name': row[2],\n",
    "            'followers_count': row[4],\n",
    "        }\n",
    "        users_dict[user_id] = user_details\n",
    "\n",
    "    return users_dict\n",
    "\n",
    "\n",
    "def find_user_by_id(user_id):\n",
    "    # This function finds users with a specific ID\n",
    "    connection = cnx\n",
    "    cursor = cnx.cursor()\n",
    "    query = \"SELECT * FROM users WHERE id = %s\"\n",
    "    cursor.execute(query, (user_id,))\n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    results = []\n",
    "    for row in cursor.fetchall():\n",
    "        results.append(dict(zip(columns, row)))\n",
    "    return results\n",
    "\n",
    "def users_10():\n",
    "    connection = cnx\n",
    "    cursor = cnx.cursor()\n",
    "    cursor.execute(\"SELECT * FROM users ORDER BY statuses_count/(DATEDIFF(NOW(), created_at)+1) DESC LIMIT 10;\")\n",
    "    rows = cursor.fetchall()\n",
    "    result = []\n",
    "    for row in rows:\n",
    "        user_dict = {\n",
    "            \"id\": row[0],\n",
    "            \"name\": row[1],\n",
    "            \"screen_name\": row[2],\n",
    "            \"statuses_count\": row[3],\n",
    "            \"created_at\": row[4]\n",
    "        }\n",
    "        result.append(user_dict)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902f6cb",
   "metadata": {},
   "source": [
    "## Creating Home Page for App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2254e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_date(date):\n",
    "    if isinstance(date, datetime):\n",
    "        return date\n",
    "    else:\n",
    "        try:\n",
    "            return datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return datetime.strptime(date, \"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f30bab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Home Page\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "\n",
    "def index():\n",
    "    if request.method=='POST':\n",
    "        search_term = request.form['search']\n",
    "        search_by = request.form['search_by']\n",
    "        start_date = \"2000-01-01\"\n",
    "        end_date = \"2030-01-01\"\n",
    "        if request.form['date_from']:\n",
    "            start_date = request.form['date_from']\n",
    "            if request.form['date_to']:\n",
    "                end_date = request.form['date_to']\n",
    "            return redirect(url_for('results',search_by = search_by, search_term = search_term, start_date = start_date, end_date = end_date))\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26b1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/results/<search_by>/<search_term>/<start_date>/<end_date>', methods=['GET','POST'])\n",
    "def results(search_by, search_term, start_date=\"2000-01-01\", end_date=\"2030-01-01\"):\n",
    "    start = time.time()\n",
    "    end = None \n",
    "    \n",
    "    start_datetime = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_datetime = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    search_key = \"search \" + search_term + \" category \" + search_by\n",
    "    if search_by == 'hashtag':\n",
    "        search_key = 'tweets_with_' + search_term\n",
    "        # Retrieve cache_results\n",
    "        cache_results = cache.get_from_cache(search_key)\n",
    "        if cache_results != -1:\n",
    "            most_recent_date = cache.get_from_cache(\"most_recent_date\")\n",
    "            three_days_ago = cache.get_from_cache(\"three_days_ago\")\n",
    "            hashtag_tweets = cache_results  # Assuming cache_results contains hashtag-related tweets\n",
    "            if con_date(most_recent_date) >= con_date(start_datetime) and con_date(three_days_ago) <= con_date(end_datetime):\n",
    "                search_results = cache_results\n",
    "            else:\n",
    "                search_key = 'search' + search_term + 'category' + search_by\n",
    "        else:\n",
    "            search_key = 'search' + search_term + 'category' + search_by\n",
    "    else:\n",
    "        search_key = 'search' + search_term + 'category' + search_by\n",
    "    cache_results = cache.get_from_cache(search_key)\n",
    "    if cache_results != -1:\n",
    "        search_results = cache_results\n",
    "    else:\n",
    "        if search_by == 'text':\n",
    "            search_results = search_tweets_by_keyword(search_term)\n",
    "            for doc in search_results:\n",
    "                user_info = find_user_by_id(doc['UserID'])\n",
    "                del doc['_id']\n",
    "                try:\n",
    "                    doc['name'] = user_info[0][2]\n",
    "                    doc['screen_name'] = user_info[0][3]\n",
    "                except:\n",
    "                    continue\n",
    "        elif search_by == \"author\":\n",
    "            user_info = find_users_with_screen_name(search_term)\n",
    "            if user_info:\n",
    "                search_results = []\n",
    "                for i in user_info:\n",
    "                    search_result = find_user_by_id(i)\n",
    "                    for doc in search_result:\n",
    "                        doc['name'] = user_info[i]['name']\n",
    "                        doc['screen_name'] = user_info[i]['screen_name']\n",
    "                        del doc['id']\n",
    "                        search_results.append(doc)\n",
    "            else:\n",
    "                search_results = []\n",
    "        elif search_by == \"hashtag\":\n",
    "            search_results = get_hashtags_in_tweets_with_keyword(search_term)\n",
    "            for doc in search_results:\n",
    "                user_info = find_user_by_id(doc['UserID'])\n",
    "                del doc['_id']\n",
    "                try:\n",
    "                    doc['name'] = user_info[0][2]\n",
    "                    doc['screen_name'] = user_info[0][3]\n",
    "                except:\n",
    "                    continue\n",
    "        cache.push_to_cache(search_key, search_results)\n",
    "    \n",
    "    # Move the filtering logic outside of the else block\n",
    "    for doc in search_results[:]:\n",
    "        #doc['created_at'] = datetime(doc['created_at'])\n",
    "        if not (con_date(start_datetime) <= con_date(doc['created_at']) <= con_date(end_datetime)):\n",
    "            search_results.remove(doc)\n",
    "    \n",
    "    if request.method =='POST':\n",
    "        sort_by = request.form['sort_by']\n",
    "        if sort_by == \"relevance\":\n",
    "            if search_by == \"author\":\n",
    "                search_results.sort(key=lambda x: x['followers_count'], reverse=True)\n",
    "            else:\n",
    "                search_results.sort(key=lambda x: x['Retweet_Count'], reverse=True)\n",
    "        elif sort_by == \"recent\":\n",
    "            search_results.sort(key=lambda x: x['created_at'], reverse=True)\n",
    "        elif sort_by == \"old\":\n",
    "            search_results.sort(key=lambda x: x['created_at'])\n",
    "        end = time.time()  # Update end time here\n",
    "        return render_template('results.html', results=search_results, search_time=end-start)\n",
    "\n",
    "    end = time.time()  # Update end time here as well\n",
    "    if len(search_results) == 0:\n",
    "        flash('NO RESULTS FOUND')\n",
    "        return redirect(url_for('index'))\n",
    "    else:\n",
    "        return render_template('results.html', results=search_results, search_time=end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce9c2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page for User Drill-down, accepts a user_id\n",
    "@app.route('/user/<user_id>')\n",
    "def user(user_id):\n",
    "    user_info = search_by_user_id(user_id)\n",
    "    if not user_info:\n",
    "        flash('USER NOT FOUND')\n",
    "        return redirect(url_for('index'))\n",
    "    search_results = author_search(user_info[0][0], sorting_var = \"created_at\")\n",
    "    return render_template('users.html', user_info = user_info, results = search_results)\n",
    "\n",
    "#Page for top 10 users by follower count\n",
    "@app.route('/top10users')\n",
    "def top_user():\n",
    "    start = time.time()\n",
    "    cache_results = cache.most_active_users()\n",
    "    if cache_results == -1:\n",
    "        top_users_ = users_10()\n",
    "        top_users=[]\n",
    "        for doc in top_users_:\n",
    "            doc = list(doc)\n",
    "            doc[9] = doc[9].strftime(\"%Y-%m-%d\")\n",
    "            top_users.append(doc)\n",
    "        cache.push_to_top10_cache('top10users',top_users)\n",
    "    else:\n",
    "        top_users = cache_results\n",
    "    end = time.time()\n",
    "    return render_template('top10users.html', top_users = top_users, search_time = end-start)\n",
    "\n",
    "#Page for top 10 tweets by retweet count\n",
    "@app.route('/top10tweets_favorite')\n",
    "def top_tweets_fav():\n",
    "    start = time.time()\n",
    "    cache_results = cache.top10_tweets()\n",
    "    if cache_results == -1:\n",
    "        top_tweets = tweets_10()\n",
    "        for doc in top_tweets:\n",
    "            del doc['_id']\n",
    "            user_info = find_by_user_id(doc['UserID'])\n",
    "            try:\n",
    "                doc['name'] = user_info[0][2]\n",
    "                doc['screen_name'] = user_info[0][3]\n",
    "            except:\n",
    "                return\n",
    "            cache.push_to_cache('top10_tweets', top_tweets)\n",
    "    else:\n",
    "        top_tweets = cache_results\n",
    "        end = time.time()\n",
    "        print(top_tweets)\n",
    "        return render_template('top10tweets_favorite.html', top_tweets = top_tweets, search_time = end-start)\n",
    "    \n",
    "@app.route('/top10tweets_retweets')\n",
    "def top_tweets_retweet():\n",
    "    start = time.time()\n",
    "    cache_results = cache.top_retweets()\n",
    "    if cache_results == -1:\n",
    "        top_tweets = tweets_10()\n",
    "        for doc in top_tweets:\n",
    "            del doc['_id']\n",
    "            user_info = find_by_user_id(doc['UserID'])\n",
    "            try:\n",
    "                doc['name'] = user_info[0][2]\n",
    "                doc['screen_name'] = user_info[0][3]\n",
    "            except:\n",
    "                return\n",
    "            cache.push_to_cache('top_retweets', top_tweets)\n",
    "    else:\n",
    "        top_tweets = cache_results\n",
    "        end = time.time()\n",
    "        return render_template('top10tweets_retweets.html', top_tweets = top_tweets, search_time = end-start)\n",
    "    \n",
    "@app.route('/top10hashtags')\n",
    "def top_hashtag():\n",
    "    start = time.time()\n",
    "    cache_results = cache.most_common_hashtags()\n",
    "    if cache_results == -1:\n",
    "        top_hashtags = hashtag_10()\n",
    "        for doc in top_hashtags:\n",
    "            cache.push_to_cache('top_hashtag', top_hashtags)\n",
    "    else:\n",
    "        top_hashtags = cache_results\n",
    "        end = time.time()\n",
    "        return render_template('top10hashtags.html', top_hashtags = top_hashtags, search_time = end-start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5002\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [26/Apr/2024 10:27:43] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:28:09] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:28:19] \"GET /results/text/corona/2020-04-01/2020-04-07 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:28:27] \"POST /results/text/corona/2020-04-01/2020-04-07 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:42] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:47] \"GET /top10users HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:49] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:50] \"GET /top10tweets_favorite HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('662b9c7022717e940d9f2322'), 'created_at': datetime.datetime(2020, 3, 4, 17, 31, 21), 'TweetID': 1235256530728972290, 'Id_str': '1235256530728972290', 'Text': 'ALERT‼️‼️‼️\\nThe corona virus can be spread through money. If you have any money at home, put on some gloves, put al… https://t.co/juJjDpFN3I', 'Full_Text': \"ALERT‼️‼️‼️\\nThe corona virus can be spread through money. If you have any money at home, put on some gloves, put all the money in to a plastic bag and put it outside the front door tonight. I'm collecting all the plastic bags tonight for safety. Think of your health.\", 'Hashtag': [], 'UserID': 2863558530, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 1128502}, {'_id': ObjectId('662b9f4222717e940d9fdf0a'), 'created_at': datetime.datetime(2020, 3, 13, 0, 43, 40), 'TweetID': 1238264431320215553, 'Id_str': '1238264431320215553', 'Text': '*corona virus enters my body*\\n\\nThe 4 Flintstone gummies I ate in 2005: https://t.co/3STfdIQtaT', 'Full_Text': '*corona virus enters my body*\\n\\nThe 4 Flintstone gummies I ate in 2005: https://t.co/3STfdIQtaT', 'Hashtag': [], 'UserID': 1100261477989126145, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 811062}, {'_id': ObjectId('662b9e7722717e940d9fb6f5'), 'created_at': datetime.datetime(2020, 3, 18, 17, 51, 18), 'TweetID': 1240334979701395458, 'Id_str': '1240334979701395458', 'Text': 'When this Corona shit passes we have to promise each other that we’re going to tell our kids that we survived a zombie apocalypse in 2020', 'Full_Text': 'When this Corona shit passes we have to promise each other that we’re going to tell our kids that we survived a zombie apocalypse in 2020', 'Hashtag': [], 'UserID': 1112592502727548928, 'Retweet_Count': 2, 'Retweet_ID': 0, 'Favorite_Count': 764405}, {'_id': ObjectId('662b9c4a22717e940d9f11de'), 'created_at': datetime.datetime(2020, 3, 13, 23, 47, 3), 'TweetID': 1238612571193827335, 'Id_str': '1238612571193827335', 'Text': 'If I gave you 100 skittles and told you 3 of them could kill you.... I’m sure you would avoid the fucking skittles', 'Full_Text': 'If I gave you 100 skittles and told you 3 of them could kill you.... I’m sure you would avoid the fucking skittles', 'Hashtag': [], 'UserID': 29942414, 'Retweet_Count': 3, 'Retweet_ID': 0, 'Favorite_Count': 598511}, {'_id': ObjectId('662b9c5122717e940d9f151e'), 'created_at': datetime.datetime(2020, 3, 13, 16, 38, 2), 'TweetID': 1238504604767223808, 'Id_str': '1238504604767223808', 'Text': 'It wasn’t no corona till y’all started balancing brooms in the house, y’all let the devil in', 'Full_Text': 'It wasn’t no corona till y’all started balancing brooms in the house, y’all let the devil in', 'Hashtag': [], 'UserID': 219582851, 'Retweet_Count': 2, 'Retweet_ID': 0, 'Favorite_Count': 572345}, {'_id': ObjectId('662b9bcc22717e940d9ed33f'), 'created_at': datetime.datetime(2020, 3, 10, 17, 52, 14), 'TweetID': 1237436114887041024, 'Id_str': '1237436114887041024', 'Text': 'THIS MAN IS A GENIUS he figured out the Corona virus problem 😮 https://t.co/EZP7IqTtxV', 'Full_Text': 'THIS MAN IS A GENIUS he figured out the Corona virus problem 😮 https://t.co/EZP7IqTtxV', 'Hashtag': [], 'UserID': 1131227186, 'Retweet_Count': 3, 'Retweet_ID': 0, 'Favorite_Count': 517026}, {'_id': ObjectId('662ba57422717e940da0bb0d'), 'created_at': datetime.datetime(2020, 3, 18, 0, 3, 4), 'TweetID': 1240066150278430720, 'Id_str': '1240066150278430720', 'Text': 'yeah it’s a new generation we gon call them quaranteens https://t.co/rJSKmIf7Qp', 'Full_Text': 'yeah it’s a new generation we gon call them quaranteens https://t.co/rJSKmIf7Qp', 'Hashtag': [], 'UserID': 825811162381967360, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 489101}, {'_id': ObjectId('662ba00d22717e940da002f8'), 'created_at': datetime.datetime(2020, 3, 16, 23, 4, 57), 'TweetID': 1239689136358985729, 'Id_str': '1239689136358985729', 'Text': '“corona time “😭😭😭😭 https://t.co/iXBMHVcFoY', 'Full_Text': '“corona time “😭😭😭😭 https://t.co/iXBMHVcFoY', 'Hashtag': [], 'UserID': 728371324184301568, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 422685}, {'_id': ObjectId('662b9bd322717e940d9ed847'), 'created_at': datetime.datetime(2020, 3, 15, 19, 8, 57), 'TweetID': 1239267360739074048, 'Id_str': '1239267360739074048', 'Text': 'Watch this. It shows why we should all do the right thing and stay home to the fullest extent possible. All of us c… https://t.co/GOODRTNI2e', 'Full_Text': 'Watch this. It shows why we should all do the right thing and stay home to the fullest extent possible. All of us can help slow the spread of the virus, protecting the elderly, the vulnerable, and each other. https://t.co/FgffQrMVB7', 'Hashtag': [], 'UserID': 813286, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 388518}, {'_id': ObjectId('662b9c5722717e940d9f17d7'), 'created_at': datetime.datetime(2020, 3, 16, 23, 34, 16), 'TweetID': 1239696517008482305, 'Id_str': '1239696517008482305', 'Text': 'we all know who the only person who can defeat corona is https://t.co/u7DraiQHHY', 'Full_Text': 'we all know who the only person who can defeat corona is https://t.co/u7DraiQHHY', 'Hashtag': [], 'UserID': 784597616269156352, 'Retweet_Count': 2, 'Retweet_ID': 0, 'Favorite_Count': 384341}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Apr/2024 10:36:53] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:54] \"GET /top10tweets_retweets HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:56] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:58] \"GET /top10hashtags HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:36:59] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:20] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:21] \"GET /results/text/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:26] \"POST /results/text/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:31] \"POST /results/text/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:36] \"POST /results/text/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:40] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:59] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:59] \"GET /results/author/sam/2010-01-26/2024-04-26 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:37:59] \"GET /static/dummy_sam.jpeg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:05] \"POST /results/author/sam/2010-01-26/2024-04-26 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:05] \"GET /static/dummy_sam.jpeg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:07] \"POST /results/author/sam/2010-01-26/2024-04-26 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:07] \"GET /static/dummy_sam.jpeg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:09] \"POST /results/author/sam/2010-01-26/2024-04-26 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:09] \"GET /static/dummy_sam.jpeg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:16] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:38] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:40] \"GET /results/hashtag/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:45] \"POST /results/hashtag/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:48] \"POST /results/hashtag/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:50] \"POST /results/hashtag/corona/2020-04-01/2020-04-05 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:38:52] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:46:25] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:46:26] \"GET /results/text/corona/2020-04-04/2020-04-07 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:46:38] \"POST /results/text/corona/2020-04-04/2020-04-07 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:46:45] \"POST /results/text/corona/2020-04-04/2020-04-07 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:47:09] \"POST /results/text/corona/2020-04-04/2020-04-07 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:47:20] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:47:32] \"GET /top10users HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:47:43] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:47:44] \"GET /top10tweets_favorite HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': ObjectId('662b9c7022717e940d9f2322'), 'created_at': datetime.datetime(2020, 3, 4, 17, 31, 21), 'TweetID': 1235256530728972290, 'Id_str': '1235256530728972290', 'Text': 'ALERT‼️‼️‼️\\nThe corona virus can be spread through money. If you have any money at home, put on some gloves, put al… https://t.co/juJjDpFN3I', 'Full_Text': \"ALERT‼️‼️‼️\\nThe corona virus can be spread through money. If you have any money at home, put on some gloves, put all the money in to a plastic bag and put it outside the front door tonight. I'm collecting all the plastic bags tonight for safety. Think of your health.\", 'Hashtag': [], 'UserID': 2863558530, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 1128502}, {'_id': ObjectId('662b9f4222717e940d9fdf0a'), 'created_at': datetime.datetime(2020, 3, 13, 0, 43, 40), 'TweetID': 1238264431320215553, 'Id_str': '1238264431320215553', 'Text': '*corona virus enters my body*\\n\\nThe 4 Flintstone gummies I ate in 2005: https://t.co/3STfdIQtaT', 'Full_Text': '*corona virus enters my body*\\n\\nThe 4 Flintstone gummies I ate in 2005: https://t.co/3STfdIQtaT', 'Hashtag': [], 'UserID': 1100261477989126145, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 811062}, {'_id': ObjectId('662b9e7722717e940d9fb6f5'), 'created_at': datetime.datetime(2020, 3, 18, 17, 51, 18), 'TweetID': 1240334979701395458, 'Id_str': '1240334979701395458', 'Text': 'When this Corona shit passes we have to promise each other that we’re going to tell our kids that we survived a zombie apocalypse in 2020', 'Full_Text': 'When this Corona shit passes we have to promise each other that we’re going to tell our kids that we survived a zombie apocalypse in 2020', 'Hashtag': [], 'UserID': 1112592502727548928, 'Retweet_Count': 2, 'Retweet_ID': 0, 'Favorite_Count': 764405}, {'_id': ObjectId('662b9c4a22717e940d9f11de'), 'created_at': datetime.datetime(2020, 3, 13, 23, 47, 3), 'TweetID': 1238612571193827335, 'Id_str': '1238612571193827335', 'Text': 'If I gave you 100 skittles and told you 3 of them could kill you.... I’m sure you would avoid the fucking skittles', 'Full_Text': 'If I gave you 100 skittles and told you 3 of them could kill you.... I’m sure you would avoid the fucking skittles', 'Hashtag': [], 'UserID': 29942414, 'Retweet_Count': 3, 'Retweet_ID': 0, 'Favorite_Count': 598511}, {'_id': ObjectId('662b9c5122717e940d9f151e'), 'created_at': datetime.datetime(2020, 3, 13, 16, 38, 2), 'TweetID': 1238504604767223808, 'Id_str': '1238504604767223808', 'Text': 'It wasn’t no corona till y’all started balancing brooms in the house, y’all let the devil in', 'Full_Text': 'It wasn’t no corona till y’all started balancing brooms in the house, y’all let the devil in', 'Hashtag': [], 'UserID': 219582851, 'Retweet_Count': 2, 'Retweet_ID': 0, 'Favorite_Count': 572345}, {'_id': ObjectId('662b9bcc22717e940d9ed33f'), 'created_at': datetime.datetime(2020, 3, 10, 17, 52, 14), 'TweetID': 1237436114887041024, 'Id_str': '1237436114887041024', 'Text': 'THIS MAN IS A GENIUS he figured out the Corona virus problem 😮 https://t.co/EZP7IqTtxV', 'Full_Text': 'THIS MAN IS A GENIUS he figured out the Corona virus problem 😮 https://t.co/EZP7IqTtxV', 'Hashtag': [], 'UserID': 1131227186, 'Retweet_Count': 3, 'Retweet_ID': 0, 'Favorite_Count': 517026}, {'_id': ObjectId('662ba57422717e940da0bb0d'), 'created_at': datetime.datetime(2020, 3, 18, 0, 3, 4), 'TweetID': 1240066150278430720, 'Id_str': '1240066150278430720', 'Text': 'yeah it’s a new generation we gon call them quaranteens https://t.co/rJSKmIf7Qp', 'Full_Text': 'yeah it’s a new generation we gon call them quaranteens https://t.co/rJSKmIf7Qp', 'Hashtag': [], 'UserID': 825811162381967360, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 489101}, {'_id': ObjectId('662ba00d22717e940da002f8'), 'created_at': datetime.datetime(2020, 3, 16, 23, 4, 57), 'TweetID': 1239689136358985729, 'Id_str': '1239689136358985729', 'Text': '“corona time “😭😭😭😭 https://t.co/iXBMHVcFoY', 'Full_Text': '“corona time “😭😭😭😭 https://t.co/iXBMHVcFoY', 'Hashtag': [], 'UserID': 728371324184301568, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 422685}, {'_id': ObjectId('662b9bd322717e940d9ed847'), 'created_at': datetime.datetime(2020, 3, 15, 19, 8, 57), 'TweetID': 1239267360739074048, 'Id_str': '1239267360739074048', 'Text': 'Watch this. It shows why we should all do the right thing and stay home to the fullest extent possible. All of us c… https://t.co/GOODRTNI2e', 'Full_Text': 'Watch this. It shows why we should all do the right thing and stay home to the fullest extent possible. All of us can help slow the spread of the virus, protecting the elderly, the vulnerable, and each other. https://t.co/FgffQrMVB7', 'Hashtag': [], 'UserID': 813286, 'Retweet_Count': 1, 'Retweet_ID': 0, 'Favorite_Count': 388518}, {'_id': ObjectId('662b9c5722717e940d9f17d7'), 'created_at': datetime.datetime(2020, 3, 16, 23, 34, 16), 'TweetID': 1239696517008482305, 'Id_str': '1239696517008482305', 'Text': 'we all know who the only person who can defeat corona is https://t.co/u7DraiQHHY', 'Full_Text': 'we all know who the only person who can defeat corona is https://t.co/u7DraiQHHY', 'Hashtag': [], 'UserID': 784597616269156352, 'Retweet_Count': 2, 'Retweet_ID': 0, 'Favorite_Count': 384341}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Apr/2024 10:47:51] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:47:52] \"GET /top10tweets_retweets HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:48:01] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:48:02] \"GET /top10hashtags HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:48:12] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:49:06] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:49:06] \"GET /results/hashtag/corona/2020-04-04/2020-04-08 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:49:24] \"POST /results/hashtag/corona/2020-04-04/2020-04-08 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:49:40] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:50:07] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:50:07] \"GET /results/author/sam/2010-01-26/2024-04-26 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:50:07] \"GET /static/dummy_sam.jpeg HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:50:54] \"POST /results/author/sam/2010-01-26/2024-04-26 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Apr/2024 10:50:54] \"GET /static/dummy_sam.jpeg HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=True, port=5002, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a4759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
